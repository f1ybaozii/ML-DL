{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP 多层感知机\n",
    "\n",
    "- 在utils中定义了sigmoid，ReLU激活函数。\n",
    "- 二分类使用NLL损失，即使用模型预测概率计算负对数似然\n",
    "  - $$\\mathcal L = -\\sum y\\log p$$\n",
    "  - 计算损失函数到输入值的梯度为$$\\frac{\\partial \\mathcal L}{\\partial p} = - \\sum y \\frac{1}{p}$$\n",
    "  - 在二分类中两者分别为inputs和1-inputs。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import numpy as np\n",
    "\n",
    "# 定义损失函数及其梯度\n",
    "class NLLLoss(object):\n",
    "    \"\"\" Negative Log Likelihood Loss\"\"\"\n",
    "    def __init__(self, reduction:str=None):\n",
    "        self.reduction = reduction\n",
    "\n",
    "        # 保留上下文用于计算梯度\n",
    "        self.ctx_inputs = None\n",
    "        self.ctx_labels = None \n",
    "\n",
    "    def forward(self, inputs, labels):\n",
    "        \"\"\" calculate loss \"\"\"\n",
    "        assert inputs.shape[0] == labels.shape[0], \\\n",
    "            f\"inputs and labels should be in same number of samples \\\n",
    "              but get {inputs.shape[0]} inputs and {labels.shape[0]} lables\"\n",
    "        assert np.min(inputs) > 0 and np.max(inputs) < 1.0, \\\n",
    "            f\"inputs should between 0 and 1, \\\n",
    "                but get value from {np.min(inputs):.4f} to {np.max(inputs):.4f}\"\n",
    "\n",
    "        if len(inputs.shape) == 2:\n",
    "            labels = labels[:, None]\n",
    "        assert inputs.shape == labels.shape\n",
    "\n",
    "        # 计算损失\n",
    "        likelihood = np.multiply(labels, np.log(inputs)) \\\n",
    "             + np.multiply((1 - labels), np.log(1 - inputs)) \n",
    "        loss = - likelihood\n",
    "\n",
    "        # 保存上下文信息\n",
    "        self.ctx_inputs = inputs\n",
    "        self.ctx_labels = labels\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return np.mean(loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return np.sum(loss)\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "    def backward(self, prev_grad):\n",
    "        \"\"\" calculate gradient \"\"\"\n",
    "        grad = np.multiply(self.ctx_labels, np.reciprocal(self.ctx_inputs)) \\\n",
    "             + np.multiply((1 - self.ctx_labels), - np.reciprocal((1 - self.ctx_inputs)))\n",
    "\n",
    "        return  -grad * prev_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于mlp中的每一层,定义一层感知器.\n",
    "包括正向传播,反向传播和梯度更新.\n",
    "\n",
    "其中正向传播公式为\n",
    "$$y = \\sigma(z) = \\sigma(w^Tx + b)$$\n",
    "其中z为输入特征的加权和,保存在ctx_hidden变量中用于计算残差\n",
    "\n",
    "反向传播过程中,首先计算残差\n",
    "$$\\frac{\\partial \\mathcal L}{\\partial z} = \\frac{\\partial \\mathcal L}{\\partial y} \\frac{\\partial y}{\\partial z}$$\n",
    "\n",
    "可以得到损失函数对权重的梯度,保存在上下文中用于梯度更新.\n",
    "同时需要返回损失函数对前一层输出的梯度,以便前一层的梯度计算."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer:\n",
    "    \"\"\" Perceptron Layer in MLP \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        c_in: int, c_out: int,\n",
    "        init_mean: float, init_var: float,\n",
    "        bias:bool = False,\n",
    "        activation: str = 'relu'\n",
    "    ) -> None:\n",
    "\n",
    "        self.bias = bias\n",
    "        if self.bias:\n",
    "            weight_size = (c_in+1, c_out)\n",
    "        else:\n",
    "            weight_size = (c_in, c_out)\n",
    "\n",
    "        self.grad = None\n",
    "        self.weight = np.random.normal(\n",
    "            init_mean, init_var, \n",
    "            size=weight_size\n",
    "        )\n",
    "        \n",
    "        self.activation = _get_activation(activation)\n",
    "\n",
    "        self.ctx_inputs = None\n",
    "        self.ctx_hidden = None\n",
    "        self.ctx_outputs = None\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\" forward the network \"\"\"\n",
    "        bsz = inputs.shape[0]\n",
    "\n",
    "        if self.bias:\n",
    "            self.ctx_inputs = np.concatenate([\n",
    "                inputs,\n",
    "                np.ones(shape=(bsz, 1))\n",
    "            ], axis=1)\n",
    "        else:\n",
    "            self.ctx_inputs = inputs\n",
    "\n",
    "        self.ctx_hidden = np.matmul(self.ctx_inputs, self.weight)\n",
    "        self.ctx_outputs = self.activation(self.ctx_hidden)\n",
    "\n",
    "        return self.ctx_outputs\n",
    "\n",
    "    def backward(self, prev_grad):\n",
    "        \"\"\" calculate the gradient \"\"\"\n",
    "        assert self.ctx_hidden.shape == prev_grad.shape, \\\n",
    "            f\"expected same shape of ctx_hidden and prev_grad, \\\n",
    "              but get ctx_hidden in {self.ctx_hidden.shape} and \\\n",
    "              prev_grad in {prev_grad.shape}\"\n",
    "\n",
    "        if self.activation.__name__ == 'relu':\n",
    "            residual = prev_grad * np.where(self.ctx_hidden >= 0, 1, 0)\n",
    "        elif self.activation.__name__ == 'sigmoid':\n",
    "            residual = prev_grad * np.multiply(self.ctx_outputs, 1 - self.ctx_outputs)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"activation not supported\")\n",
    "\n",
    "        self.grad = np.matmul(self.ctx_inputs.transpose(1, 0), residual)\n",
    "\n",
    "        if self.bias:\n",
    "            return np.matmul(residual, self.weight.transpose(1, 0))[:, :-1]\n",
    "        else:\n",
    "            return np.matmul(residual, self.weight.transpose(1, 0))\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        \"\"\" update weight with the gradient \"\"\"\n",
    "        assert self.grad is not None\n",
    "\n",
    "        self.weight = self.weight - learning_rate * self.grad\n",
    "        self.grad = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "d2l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
